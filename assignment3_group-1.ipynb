{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 0, 0: 1126, 1: 1010, 2: 1144, 3: 1196, 4: 957, 5: 1204, 6: 1090, 7: 1013, 8: 1162, 9: 0, 10: 1114, 11: 1241, 12: 1055, 13: 1151, 14: 1196, 15: 1088, 16: 1279, 17: 1294, 18: 1199, 19: 1186, 20: 1161, 21: 1082, 22: 1225, 23: 1164, 24: 1118, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0}\n",
      "X_train numpy shape: (27455, 784)\n",
      "X_test numpy shape: (7172, 784)\n",
      "y_train numpy shape: (27455,)\n",
      "y_test numpy shape: (7172,)\n",
      "X_train tensor shape: torch.Size([27455, 28, 28, 1])\n",
      "X_test tensor shape: torch.Size([7172, 28, 28, 1])\n",
      "y_train tensor shape: torch.Size([27455])\n",
      "y_test tensor shape: torch.Size([7172])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df_train = pd.read_csv('./sign_mnist_train.csv')\n",
    "df_test = pd.read_csv(\"./sign_mnist_test.csv\")\n",
    "\n",
    "X_train, y_train = df_train.iloc[:,1:].values/255.0, df_train.iloc[:,0].values #normalizing takes place here\n",
    "X_test, y_test = df_test.iloc[:,1:].values/255.0, df_test.iloc[:,0].values\n",
    "\n",
    "value_counts = {value: np.count_nonzero(y_train == value) for value in range(-1, 31)}\n",
    "\n",
    "print(value_counts)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer() #One hot encoding of the labels\n",
    "# y_train = lb.fit_transform(y_train)\n",
    "# y_test = lb.fit_transform(y_test)\n",
    "\n",
    "print(\"X_train numpy shape: \"+ str(X_train.shape))\n",
    "print(\"X_test numpy shape: \" +str(X_test.shape))\n",
    "print(\"y_train numpy shape: \" +str(y_train.shape))\n",
    "print(\"y_test numpy shape: \" + str(y_test.shape))\n",
    "\n",
    "\n",
    "X_train_tensor, y_train_tensor = torch.tensor(X_train).reshape(-1, 28, 28, 1), torch.tensor(y_train) #Reshaped to 2D images for the CNN\n",
    "X_test_tensor, y_test_tensor = torch.tensor(X_test).reshape(-1,28,28,1), torch.tensor(y_test)\n",
    "\n",
    "print(\"X_train tensor shape: \"+ str(X_train_tensor.shape))\n",
    "print(\"X_test tensor shape: \" +str(X_test_tensor.shape))\n",
    "print(\"y_train tensor shape: \" +str(y_train_tensor.shape))\n",
    "print(\"y_test tensor shape: \" + str(y_test_tensor.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y_true, predicted_probabilities):\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    correct_predictions = predicted_classes == y_true\n",
    "    accuracy = np.mean(correct_predictions) \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        #dw = gradient.dot(self.cur_input)\n",
    "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
    "        db = gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.w.copy(), self.b.copy()\n",
    "    \n",
    "    def set_params(self, w :np.ndarray, b:np.ndarray):\n",
    "        self.w = w.copy()\n",
    "        self.b = b.copy()\n",
    "    \n",
    "\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "    \n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shift x by subtracting its max value from each vector in the batch\n",
    "        shift_x = x - np.max(x, axis=-1, keepdims=True)\n",
    "        exps = np.exp(shift_x)\n",
    "        probs = exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, *args: List[NeuralNetLayer]):\n",
    "        self.layers = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)\n",
    "\n",
    "    def get_params(self):\n",
    "        # Collect parameters from all layers that have them\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'get_params'):\n",
    "                params.extend(layer.get_params())\n",
    "        return params\n",
    "\n",
    "    def apply_params(self, parameters):\n",
    "        # Apply parameters to all layers that have them\n",
    "        param_iter = iter(parameters)\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_params'):\n",
    "                layer.set_params(next(param_iter), next(param_iter))\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, net: MLP):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            g_avg = g.mean(axis=0)\n",
    "            p -= self.lr * g_avg\n",
    "\n",
    "\n",
    "class GradientDescentOptimizerWithSchedule(GradientDescentOptimizer):\n",
    "    def __init__(self, net: MLP, lr: float, schedule_step: int, decay_factor: float):\n",
    "        super().__init__(net, lr)\n",
    "        self.schedule_step = schedule_step\n",
    "        self.decay_factor = decay_factor\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self):\n",
    "        super().step()\n",
    "        self.current_step += 1\n",
    "        if self.current_step % self.schedule_step == 0:\n",
    "            self.update_lr()\n",
    "\n",
    "    def update_lr(self):\n",
    "        self.lr *= self.decay_factor\n",
    "        print(f\"Updated learning rate to {self.lr}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(mlp: MLP, optimizer: Optimizer, data_x, data_y, steps, batch_size=128, patience =250):\n",
    "\n",
    "#     #First split into train/validation split\n",
    "#     np.random.seed(13) #So that its deterministic\n",
    "#     val_size = int(len(data_x) * 0.2)  # For an 80/20 split\n",
    "#     indices = np.arange(len(data_x))\n",
    "#     np.random.shuffle(indices)\n",
    "#     val_indices = indices[:val_size]\n",
    "#     train_indices = indices[val_size:]\n",
    "#     # Use the indices to create training and validation sets\n",
    "#     x_train, y_train = data_x[train_indices], data_y[train_indices]\n",
    "#     x_val, y_val = data_x[val_indices], data_y[val_indices]\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     #Validation stuff\n",
    "#     val_losses = [10e10]\n",
    "#     n_samples = x_train.shape[0]\n",
    "#     labels_train = np.eye(25)[np.array(y_train)]  # One-hot encode labels\n",
    "#     labels_val = np.eye(25)[np.array(y_val)]\n",
    "#     val_every_ith_epoch = 250\n",
    "#     best_params = None\n",
    "\n",
    "#     #Early stoping stuff\n",
    "#     best_loss = float('inf')\n",
    "#     no_improvement_count = 0\n",
    "\n",
    "#     for i in tqdm(range(steps)):\n",
    "#         # Randomly select batch indices for each step\n",
    "#         batch_indices = np.random.choice(n_samples, batch_size, replace=False)\n",
    "#         batch_x = x_train[batch_indices]\n",
    "#         labels = labels_train[batch_indices]\n",
    "\n",
    "#         predictions = mlp.forward(batch_x)\n",
    "        \n",
    "#         loss = -(labels * np.log(predictions+1e-9)).sum(axis=-1).mean()\n",
    "#         losses.append(loss)\n",
    "        \n",
    "#         mlp.backward(labels)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         #Early Stopping check\n",
    "#         if len(losses) > 2 * patience:  # Ensure there are enough points to compare\n",
    "#             recent_avg_loss = np.mean(losses[-patience:])\n",
    "#             prev_avg_loss = np.mean(losses[-2*patience:-patience])\n",
    "#             if recent_avg_loss < best_loss:\n",
    "#                 best_loss = recent_avg_loss\n",
    "#                 no_improvement_count = 0  # Reset the counter as we found a better loss\n",
    "#             elif recent_avg_loss >= prev_avg_loss: #no improvement\n",
    "#                 no_improvement_count += 1\n",
    "#             else:\n",
    "#                 no_improvement_count = 0  # Safeguard\n",
    "\n",
    "#             if no_improvement_count >= patience:\n",
    "#                 print(f\"Early stopping at step {i}: no improvement in the last {patience} checks.\")\n",
    "#                 break  # Stop training if no improvement for 'patience' consecutive checks\n",
    "\n",
    "#         #Validation check -> Sees all validation sets\n",
    "#         if (i % val_every_ith_epoch == 0):\n",
    "#             val_batch_size = 64\n",
    "#             batch_val_losses = []\n",
    "            \n",
    "#             # Iterate over the validation set in mini-batches\n",
    "#             for j in range(0, len(x_val), val_batch_size):\n",
    "#                 x_val_batch = x_val[j:j + val_batch_size]\n",
    "#                 y_val_batch = labels_val[j:j + val_batch_size]\n",
    "                \n",
    "#                 predictions_val_batch = mlp.forward(x_val_batch)\n",
    "                \n",
    "#                 # Compute loss for the validation batch\n",
    "#                 loss_val_batch = -(y_val_batch * np.log(predictions_val_batch + 1e-9)).sum(axis=-1).mean()\n",
    "#                 batch_val_losses.append(loss_val_batch)\n",
    "            \n",
    "#             # Calculate the average validation loss over all mini-batches\n",
    "#             loss_val = np.mean(batch_val_losses)\n",
    "\n",
    "#             if loss_val < np.min(val_losses):\n",
    "#                 best_params = mlp.get_params()\n",
    "#             val_losses.extend([loss_val] * val_every_ith_epoch) #Straight line extension\n",
    "\n",
    "#     plt.plot(losses, label='Training Loss', linestyle='-', color='blue')\n",
    "#     plt.plot(val_losses[1:], label='Validation Loss', linestyle='--', color='orange')\n",
    "#     plt.xlabel(\"Epoch\")\n",
    "#     plt.ylabel(\"Cross Entropy Loss\")\n",
    "#     plt.yscale(\"log\")\n",
    "#     plt.legend()\n",
    "#     plt.title(\"Training vs Validation Loss\")\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "#     return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###New training algorithm\n",
    "def train(mlp :MLP, optimizer, data_x, data_y, num_epochs, batch_size=128, patience=5):\n",
    "    \n",
    "    #First split into train/validation split\n",
    "    np.random.seed(13) #So that its deterministic\n",
    "    val_size = int(len(data_x) * 0.2)  # For an 80/20 split\n",
    "    indices = np.arange(len(data_x))\n",
    "    np.random.shuffle(indices)\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    # Use the indices to create training and validation sets\n",
    "    x_train, y_train = data_x[train_indices], data_y[train_indices]\n",
    "    x_val, y_val = data_x[val_indices], data_y[val_indices]\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    val_every_ith_epoch = 10\n",
    "    val_losses = [float('inf')]\n",
    "    labels_train = np.eye(25)[y_train]\n",
    "    labels_val = np.eye(25)[y_val]\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    best_params = None\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        #Shuffle training every epoch so we visit in different order\n",
    "        permutation = np.random.permutation(len(x_train))\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = labels_train[permutation]\n",
    "\n",
    "        #Now we go through every mini batch\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            batch_x = x_train_shuffled[i:i+batch_size]\n",
    "            batch_y = y_train_shuffled[i+i+batch_size]\n",
    "\n",
    "            predictions = mlp.forward(batch_x)\n",
    "            loss = -(batch_y*np.log(predictions+1e-9)).sum(axis=-1).mean() #we take mean so loss is independent of batch size?\n",
    "            losses.append(loss)\n",
    "\n",
    "            mlp.backward(batch_y)\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        #Validation check\n",
    "        if epoch % val_every_ith_epoch == 0:\n",
    "            predictions_val = mlp.forward(x_val)\n",
    "            loss_val = -(labels_val * np.log(predictions_val + 1e-9)).sum(axis=-1).mean()\n",
    "            if loss_val< np.min(val_losses):\n",
    "                best_params = mlp.get_params()\n",
    "            val_losses.append(loss_val)\n",
    "\n",
    "        #Early stopping check\n",
    "        if len(losses) > 2*patience:\n",
    "            if not check_improvement(losses, patience, best_loss):\n",
    "                no_improvement_count+=1\n",
    "                if no_improvement_count>patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}: no improvement in the last {patience} checks.\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "    \n",
    "    return best_params, losses, val_losses\n",
    "    \n",
    "    \n",
    "def check_improvement(losses, patience, best_loss):\n",
    "    recent_avg_loss = np.mean(losses[-patience:])\n",
    "    prev_avg_loss = np.mean(losses[-2*patience:-patience])\n",
    "    if recent_avg_loss<best_loss:\n",
    "        best_loss = recent_avg_loss\n",
    "        return True\n",
    "    return recent_avg_loss < prev_avg_loss\n",
    "\n",
    "\n",
    "def plot_losses(losses, val_losses):\n",
    "    plt.plot(losses, label='Training Loss', linestyle='-', color='blue')\n",
    "    plt.plot(val_losses[1:], label='Validation Loss', linestyle='--', color='orange')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross Entropy Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_stats(mlp :MLP, best_params_no_val, best_params_val, X_test, y_test, X_train, y_train):\n",
    "    mlp.apply_params(best_params_no_val)\n",
    "    accuracy_no_val_train = evaluate_accuracy(y_train, mlp.forward(X_train))*100\n",
    "    accuracy_no_val_test = evaluate_accuracy(y_test, mlp.forward(X_test))*100\n",
    "\n",
    "    mlp.apply_params(best_params_val)\n",
    "    accuracy_val_train = evaluate_accuracy(y_train, mlp.forward(X_train))*100\n",
    "    accuracy_val_test = evaluate_accuracy(y_test, mlp.forward(X_test))*100\n",
    "\n",
    "    print(f\"Train accuracy no validation = {accuracy_no_val_train:.2f}%\")\n",
    "    print(f\"Test accuracy no validation = {accuracy_no_val_test:.2f}%\")\n",
    "    print(f\"Train accuracy with validation = {accuracy_val_train:.2f}%\")\n",
    "    print(f\"Test accuracy with validation = {accuracy_val_test:.2f}%\")\n",
    "    return accuracy_no_val_train,accuracy_no_val_test,accuracy_val_train, accuracy_val_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Task 3.1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> No hidden layer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_no_hidden_layer = MLP(\n",
    "    LinearLayer(784, 25),\n",
    "    SoftmaxOutputLayer()\n",
    ")\n",
    "optimizer = GradientDescentOptimizer(mlp_no_hidden_layer, 1e-1)\n",
    "gradient_steps = 10000\n",
    "best_val_params, train_losses, val_losses = train(mlp_no_hidden_layer, optimizer, X_train, y_train, gradient_steps)\n",
    "best_no_val_params = mlp_no_hidden_layer.get_params()\n",
    "plot_losses(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NO HIDDEN LAYER\")\n",
    "no_hidden_layer_stats = get_accuracy_stats(mlp_no_hidden_layer, best_no_val_params, best_val_params, X_test, y_test, X_train, y_train)\n",
    "print(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> One Hidden Layer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter optimization\n",
    "\n",
    "\"\"\"We might be able to use the scikit-learn MLP implementation to find out which model parameters are ideal.\n",
    "In this way, we perform a grid search of all possible combinations, and then we basically report the one that's the best. Then, using the best\n",
    "model structure from the scikit-learn MLP implementation, we can train our MLP implementation and see if the accuracy is similar. It should be noted\n",
    "that max_iter refers to the number of times each data point is seen, whereas our max_iter is the number of gradient steps taken (number of batches seen) rather\n",
    "than the number of times the entire dataset has been run over.\n",
    "\n",
    "We could change this later I think...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(32,), (64,), (128,), (256,)],  # Single layer with varying sizes\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "#No regularization alpha = 0.0\n",
    "scimlp = MLPClassifier(activation='relu', solver='sgd', alpha=0.0, batch_size=128, max_iter=200)\n",
    "grid_search = GridSearchCV(scimlp, param_grid, cv=5, verbose=2)  #cv is K-fold cross validation, verbose is amount of detail printed to console\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 32\n",
    "mlp_one_hidden_layer_32 = MLP(\n",
    "    LinearLayer(784, HIDDEN_UNITS),\n",
    "    ReLULayer(),\n",
    "    LinearLayer(HIDDEN_UNITS, 25),\n",
    "    SoftmaxOutputLayer()\n",
    ")\n",
    "optimizer = GradientDescentOptimizer(mlp_one_hidden_layer_32, 1e-1)\n",
    "gradient_steps = 10000\n",
    "best_val_params_32 = train(mlp_one_hidden_layer_32, optimizer, X_train, y_train, gradient_steps)\n",
    "best_no_val_params_32 = mlp_one_hidden_layer_32.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ONE HIDDEN LAYER 32\")\n",
    "one_hidden_layer_stats = get_accuracy_stats(mlp_one_hidden_layer_32, best_no_val_params_32, best_val_params_32, X_test, y_test, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 64\n",
    "mlp_one_hidden_layer_64 = MLP(\n",
    "    LinearLayer(784, HIDDEN_UNITS),\n",
    "    ReLULayer(),\n",
    "    LinearLayer(HIDDEN_UNITS, 25),\n",
    "    SoftmaxOutputLayer()\n",
    ")\n",
    "optimizer = GradientDescentOptimizer(mlp_one_hidden_layer_64, 1e-1)\n",
    "gradient_steps = 10000\n",
    "best_val_params_64 = train(mlp_one_hidden_layer_64, optimizer, X_train, y_train, gradient_steps)\n",
    "best_no_val_params_64 = mlp_one_hidden_layer_64.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ONE HIDDEN LAYER 64\")\n",
    "one_hidden_layer_stats = get_accuracy_stats(mlp_one_hidden_layer_64, best_no_val_params_64, best_val_params_64, X_test, y_test, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 128\n",
    "mlp_one_hidden_layer_128 = MLP(\n",
    "    LinearLayer(784, HIDDEN_UNITS),\n",
    "    ReLULayer(),\n",
    "    LinearLayer(HIDDEN_UNITS, 25),\n",
    "    SoftmaxOutputLayer()\n",
    ")\n",
    "# optimizer = GradientDescentOptimizer(mlp_one_hidden_layer_128, 1e-1)\n",
    "optimizer = GradientDescentOptimizerWithSchedule(mlp_one_hidden_layer_128, lr= 0.1, schedule_step=2000, decay_factor=0.9 )\n",
    "gradient_steps = 10000\n",
    "best_val_params_128 = train(mlp_one_hidden_layer_128, optimizer, X_train, y_train, gradient_steps)\n",
    "best_no_val_params_128 = mlp_one_hidden_layer_128.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to train another 10000 epochs -> highlights the convergence problem we're seeing\n",
    "gradient_steps = 10000\n",
    "best_val_params_128 = train(mlp_one_hidden_layer_128, optimizer, X_train, y_train, gradient_steps)\n",
    "best_no_val_params_128 = mlp_one_hidden_layer_128.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ONE HIDDEN LAYER 128\")\n",
    "one_hidden_layer_stats = get_accuracy_stats(mlp_one_hidden_layer_128, best_no_val_params_128, best_val_params_128, X_test, y_test, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 256\n",
    "mlp_one_hidden_layer_256 = MLP(\n",
    "    LinearLayer(784, HIDDEN_UNITS),\n",
    "    ReLULayer(),\n",
    "    LinearLayer(HIDDEN_UNITS, 25),\n",
    "    SoftmaxOutputLayer()\n",
    ")\n",
    "optimizer = GradientDescentOptimizer(mlp_one_hidden_layer_256, 1e-1)\n",
    "gradient_steps = 10000\n",
    "best_val_params_256 = train(mlp_one_hidden_layer_256, optimizer, X_train, y_train, gradient_steps)\n",
    "best_no_val_params_256 = mlp_one_hidden_layer_256.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ONE HIDDEN LAYER 256\")\n",
    "one_hidden_layer_stats = get_accuracy_stats(mlp_one_hidden_layer_256, best_no_val_params_256, best_val_params_128, X_test, y_test, X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrITPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
