{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Verify(expression, message):\n",
    "    assert expression\n",
    "\n",
    "#Can be changed later\n",
    "def initialize_matrix(shape):\n",
    "     return np.ones(shape)\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return e_z / np.sum(e_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####THIS CELL IS NOT TO BE USED, PUT HERE SO I CAN LOOK BACK LATER MAYBE####\n",
    "\n",
    "class ReLU:\n",
    "    def __call__(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def grad(self, X):\n",
    "        return (X > 0).astype(X.dtype)\n",
    "\n",
    "## MLP Implementation uses softmax and a cross entropy loss function\n",
    "class MLP:\n",
    "    def __init__(self, activation_functor_with_grad, number_of_hidden_layers, number_of_hidden_units : list, input_dimensions=784, output_dimensions = 25, use_bias = True):\n",
    "        try:\n",
    "            Verify(number_of_hidden_layers == len(number_of_hidden_units), \"Hidden layers does not match size of hidden units list.\")\n",
    "            Verify(callable(activation_functor_with_grad) and hasattr(activation_functor_with_grad, 'grad'), \"Activation functor passed must be callable and have a grad method.\")\n",
    "\n",
    "            self.activator = activation_functor_with_grad\n",
    "            self.number_of_hidden_layers = number_of_hidden_layers\n",
    "            self.number_of_hidden_units = number_of_hidden_units\n",
    "            self.use_bias = use_bias\n",
    "\n",
    "            self.dimensions_list = [input_dimensions] + number_of_hidden_units + [output_dimensions]\n",
    "\n",
    "            self.weights = []\n",
    "            for i in range(len(self.dimensions_list)-1):\n",
    "                self.weights.append(initialize_matrix((self.dimensions_list[i]+self.use_bias, self.dimensions_list[i+1])))\n",
    "\n",
    "        except AssertionError as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def predict(self, input):\n",
    "        if self.use_bias:\n",
    "            # Adding a bias column of ones to the input if use_bias is True\n",
    "            input = np.hstack((input, np.ones((input.shape[0], 1))))\n",
    "            \n",
    "        current_layer = input\n",
    "        for weight_matrix in self.weights[:-1]:  # Excluding the last weight matrix so that we can avoid adding the bias to the final output\n",
    "            current_layer_unactivated = current_layer @ weight_matrix\n",
    "            current_layer = self.activator(current_layer_unactivated)\n",
    "            if self.use_bias:\n",
    "                # Add bias term for the next layer\n",
    "                current_layer = np.hstack((current_layer, np.ones((current_layer.shape[0], 1))))\n",
    "\n",
    "        # Processing the last layer without adding a bias term again\n",
    "        final_unactivated = current_layer @ self.weights[-1]\n",
    "        return softmax(self.activator(final_unactivated))\n",
    "    \n",
    "    def grad():\n",
    "        #This is hella complicated, trying a different approach.\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            p -= self.lr * g.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base class for neural net layers, from: https://colab.research.google.com/github/yueliyl/comp551-notebooks/blob/master/NumpyDeepMLP.ipynb\n",
    "#Overall approach is very similar to the one taken in most machine learning APIs like TensorFlow and PyTorch\n",
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "#Linear Layer implementation, with gaussian initialization\n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return (self.w[None, :,:] @ x[:,:,None]).squeeze() + self.b #Clever notation from the collab to deal with dimensions -> Final output is output_sizex1\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        Verify(self.cur_input is not None, \"Must call forawrd before backward\")\n",
    "        dw = gradient[:,:,None] @ self.cur_input[:,None,:] #Derivative with respect to the weight is the input, so we multiply the backpropped gradient by the current input\n",
    "        db = gradient #Derivative with respect to b is 1, so we just keep the previous gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "    \n",
    "\n",
    "#ReLU Layer implementation\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    #The gradient depends on what is passed in due to the discontinuity\n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x>0, 1.0, 0.0) #Gradient is 1 for input greater than 0, otherwise its 0\n",
    "    \n",
    "    def backward(self,gradient):\n",
    "        Verify(self.gradient is not None, \"Must call forward before backward\")\n",
    "        return gradient *self.gradient\n",
    "\n",
    "    def copy():\n",
    "        return ReLULayer()\n",
    "    \n",
    "#Softmax layer - gradient only valid for use with cross entropy loss\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x)\n",
    "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "    \n",
    "    def backward(self, target):\n",
    "        Verify(self.cur_probs is not None, \"Must call forward before backward\")\n",
    "        return self.cur_probs - target #Really simple gradient form when softmax is combined with cross entropy loss function\n",
    "    \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, activation_functor, number_of_hidden_layers, number_of_hidden_units : list, input_dimensions=784, output_dimensions = 25):\n",
    "        Verify(number_of_hidden_layers == len(number_of_hidden_units), \"Hidden layers does not match size of hidden units list.\")\n",
    "\n",
    "        self.activator = activation_functor\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_hidden_units = number_of_hidden_units\n",
    "\n",
    "        self.dimensions_list = [input_dimensions] + number_of_hidden_units + [output_dimensions]\n",
    "        self.layers = []\n",
    "        for i in range(len(self.dimensions_list)-1):\n",
    "            self.layers.append(LinearLayer(self.dimensions_list[i], self.dimensions_list[i+1]))\n",
    "            self.layers.append(activation_functor.copy())\n",
    "        self.layers.append(SoftmaxOutputLayer()) #Final layer is a softmax\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target) #target points to the gradients now\n",
    "\n",
    "    #Just a wrapper for convenience with other code\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def fit(self, x, y, max_iter, optimizer : Optimizer):\n",
    "        losses = []\n",
    "        labels = np.eye(26)[np.array(y)] ##TODO implement this for our output labels, 3 replaced with 26? -> Converts to onehot encoded matrix\n",
    "\n",
    "        for i in tqdm(range(max_iter)):\n",
    "            predictions = self.forward(x)\n",
    "            loss = -(labels * np.log(predictions))*sum(axis=-1).mean() #Cross entropy\n",
    "            losses.append(loss)\n",
    "            self.backward(labels) #labels now points to gradient\n",
    "            optimizer.step()\n",
    "\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Cross entropy loss\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrITPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
