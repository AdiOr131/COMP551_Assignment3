{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Verify(expression, message):\n",
    "    assert expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train numpy shape: (27455, 784)\n",
      "X_test numpy shape: (7172, 784)\n",
      "y_train numpy shape: (27455, 24)\n",
      "y_test numpy shape: (7172, 24)\n",
      "X_train tensor shape: torch.Size([27455, 28, 28, 1])\n",
      "X_test tensor shape: torch.Size([7172, 28, 28, 1])\n",
      "y_train tensor shape: torch.Size([27455, 24])\n",
      "y_test tensor shape: torch.Size([7172, 24])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df_train = pd.read_csv('./sign_mnist_train.csv')\n",
    "df_test = pd.read_csv(\"./sign_mnist_test.csv\")\n",
    "\n",
    "X_train, y_train = df_train.iloc[:,1:].values/255.0, df_train.iloc[:,0].values #normalizing takes place here\n",
    "X_test, y_test = df_test.iloc[:,1:].values/255.0, df_test.iloc[:,0].values\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer() #One hot encoding of the labels\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.fit_transform(y_test)\n",
    "\n",
    "print(\"X_train numpy shape: \"+ str(X_train.shape))\n",
    "print(\"X_test numpy shape: \" +str(X_test.shape))\n",
    "print(\"y_train numpy shape: \" +str(y_train.shape))\n",
    "print(\"y_test numpy shape: \" + str(y_test.shape))\n",
    "\n",
    "\n",
    "X_train_tensor, y_train_tensor = torch.tensor(X_train).reshape(-1, 28, 28, 1), torch.tensor(y_train) #Reshaped to 2D images for the CNN\n",
    "X_test_tensor, y_test_tensor = torch.tensor(X_test).reshape(-1,28,28,1), torch.tensor(y_test)\n",
    "\n",
    "print(\"X_train tensor shape: \"+ str(X_train_tensor.shape))\n",
    "print(\"X_test tensor shape: \" +str(X_test_tensor.shape))\n",
    "print(\"y_train tensor shape: \" +str(y_train_tensor.shape))\n",
    "print(\"y_test tensor shape: \" + str(y_test_tensor.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base class for neural net layers, from: https://colab.research.google.com/github/yueliyl/comp551-notebooks/blob/master/NumpyDeepMLP.ipynb\n",
    "#Overall approach is very similar to the one taken in most machine learning APIs like TensorFlow and PyTorch\n",
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "\n",
    "#Linear Layer implementation, with gaussian initialization\n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        return (self.w[None, :,:] @ x[:,:,None]).squeeze() + self.b #Clever notation from the collab to deal with dimensions -> Final output is output_sizex1\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        Verify(self.cur_input is not None, \"Must call forawrd before backward\")\n",
    "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]#Derivative with respect to the weight is the input, so we multiply the backpropped gradient by the current input\n",
    "        db = gradient #Derivative with respect to b is 1, so we just keep the previous gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.w, self.b\n",
    "    \n",
    "    def set_params(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    \n",
    "\n",
    "#ReLU Layer implementation\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    #The gradient depends on what is passed in due to the discontinuity\n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x>0, 1.0, 0.0) #Gradient is 1 for input greater than 0, otherwise its 0\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self,gradient):\n",
    "        Verify(self.gradient is not None, \"Must call forward before backward\")\n",
    "        return gradient *self.gradient\n",
    "\n",
    "    def copy(self):\n",
    "        return ReLULayer()\n",
    "    \n",
    "#Softmax layer - gradient only valid for use with cross entropy loss\n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x)\n",
    "        probs = exps / np.sum(exps, axis=-1)[:, None]\n",
    "        self.cur_probs = probs\n",
    "        return probs\n",
    "    \n",
    "    def backward(self, target):\n",
    "        Verify(self.cur_probs is not None, \"Must call forward before backward\")\n",
    "        return self.cur_probs - target #Really simple gradient form when softmax is combined with cross entropy loss function        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(y_true_onehot, predicted_probabilities):\n",
    "    # Convert one-hot encoded true labels to class indices\n",
    "    y_true_indices = np.argmax(y_true_onehot, axis=1)\n",
    "    \n",
    "    # Convert predicted probabilities to predicted class indices\n",
    "    predicted_indices = np.argmax(predicted_probabilities, axis=1)\n",
    "    \n",
    "    # Calculate accuracy as the mean of correct predictions\n",
    "    accuracy = np.mean(y_true_indices == predicted_indices)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, activation_layer_type, number_of_hidden_layers, number_of_hidden_units : list, input_dimensions = 784, output_dimensions = 25):\n",
    "        Verify(number_of_hidden_layers == len(number_of_hidden_units), \"Hidden layers does not match size of hidden units list.\")\n",
    "\n",
    "        self.activator = activation_layer_type\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        self.number_of_hidden_units = number_of_hidden_units\n",
    "\n",
    "        self.dimensions_list = [input_dimensions] + number_of_hidden_units + [output_dimensions]\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(len(self.dimensions_list)-1):\n",
    "            linear_layer_to_append = LinearLayer(self.dimensions_list[i], self.dimensions_list[i+1])\n",
    "            self.layers.append(linear_layer_to_append)\n",
    "            self.layers.append(activation_layer_type.copy())\n",
    "        self.layers.append(SoftmaxOutputLayer()) #Final layer is a softmax\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        # Collect parameters from all layers that have them\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'get_params'):\n",
    "                params.extend(layer.get_params())\n",
    "        return params\n",
    "\n",
    "    def apply_params(self, parameters):\n",
    "        # Apply parameters to all layers that have them\n",
    "        param_iter = iter(parameters)\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'set_params'):\n",
    "                layer.set_params(next(param_iter), next(param_iter))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    #Just a wrapper for convenience with other code\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target) #target points to the gradients now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_MLP_fancypants(mlp_to_optimize:MLP, X, y, lr, max_iter):\n",
    "    #First split into train/validation split\n",
    "    np.random.seed(13) #So that its deterministic\n",
    "    val_size = int(len(X) * 0.2)  # For an 80/20 split\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    val_indices = indices[:val_size]\n",
    "    train_indices = indices[val_size:]\n",
    "    # Use the indices to create training and validation sets\n",
    "    x_train, y_train = X[train_indices], y[train_indices]\n",
    "    x_val, y_val = X[val_indices], y[val_indices]\n",
    "\n",
    "    validation_mlp = MLP(mlp_to_optimize.activator, mlp_to_optimize.number_of_hidden_layers, mlp_to_optimize.number_of_hidden_units)\n",
    "    validation_mlp.apply_params(mlp_to_optimize.get_params())\n",
    "\n",
    "    #Setting up a validation starting point\n",
    "    val_losses = []\n",
    "    val_labels = y_val.copy()\n",
    "    val_pred = validation_mlp.predict(x_val)\n",
    "    val_loss = -(val_labels * np.log(val_pred)).sum(axis=-1).mean()\n",
    "    val_losses.append(val_loss) #need a first value to compare to\n",
    "    best_params = validation_mlp.get_params()\n",
    "\n",
    "    train_losses = []\n",
    "    train_labels = y_train.copy()\n",
    "    \n",
    "    batch_size = 128\n",
    "\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        permutation = np.random.permutation(len(x_train))\n",
    "        x_train_shuffled = x_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "\n",
    "        # Initialize variables to accumulate batch losses and accuracies\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "\n",
    "        # Mini-batch training\n",
    "        for j in range(0, len(x_train), batch_size):\n",
    "            # Create mini-batch data\n",
    "            x_batch = x_train_shuffled[j:j + batch_size]\n",
    "            y_batch = y_train_shuffled[j:j + batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            train_pred = mlp_to_optimize.forward(x_batch)\n",
    "\n",
    "            # Calculate accuracy and loss for the batch\n",
    "            batch_accuracy = evaluate_accuracy(y_batch, train_pred) * 100\n",
    "            batch_losses.append(-(y_batch * np.log(train_pred)).sum(axis=-1).mean())\n",
    "            batch_accuracies.append(batch_accuracy)\n",
    "\n",
    "            # Backward pass\n",
    "            mlp_to_optimize.backward(y_batch)\n",
    "\n",
    "            # Update weights\n",
    "            for layer in mlp_to_optimize.layers[::-1]:\n",
    "                if layer.parameters is not None:\n",
    "                    for (p, g) in zip(layer.parameters, layer.gradient):\n",
    "                        # Update parameters using the gradients from the batch\n",
    "                        # Note that we don't need to take the mean of gradients since they are already\n",
    "                        # calculated for this batch\n",
    "                        p -= lr * g.mean(axis=0)\n",
    "\n",
    "        # Calculate and print the average loss and accuracy for the epoch\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        epoch_accuracy = np.mean(batch_accuracies)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # print(f\"Accuracy at epoch {i} = {epoch_accuracy}%\")\n",
    "        # print(f\"Loss at epoch {i}: {epoch_loss}\")\n",
    "\n",
    "    best_params = mlp_to_optimize.get_params() ##TODO Change to include validation loop\n",
    "    y_pred = mlp_to_optimize.predict(x_train)\n",
    "    print(train_losses)\n",
    "    print(f\"Final Accuracy = {evaluate_accuracy(y_train, y_pred)*100}%\")\n",
    "    return best_params\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m relu \u001b[38;5;241m=\u001b[39m ReLULayer()\n\u001b[1;32m      2\u001b[0m mlp1 \u001b[38;5;241m=\u001b[39m MLP(relu, \u001b[38;5;241m2\u001b[39m, [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m30\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43moptimize_MLP_fancypants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[118], line 3\u001b[0m, in \u001b[0;36moptimize_MLP_fancypants\u001b[0;34m(mlp_to_optimize, X, y, lr, max_iter)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_MLP_fancypants\u001b[39m(mlp_to_optimize:MLP, X, y, lr, max_iter):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#First split into train/validation split\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m13\u001b[39m) \u001b[38;5;66;03m#So that its deterministic\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m# For an 80/20 split\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(X))\n",
      "Cell \u001b[0;32mIn[118], line 3\u001b[0m, in \u001b[0;36moptimize_MLP_fancypants\u001b[0;34m(mlp_to_optimize, X, y, lr, max_iter)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize_MLP_fancypants\u001b[39m(mlp_to_optimize:MLP, X, y, lr, max_iter):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#First split into train/validation split\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m13\u001b[39m) \u001b[38;5;66;03m#So that its deterministic\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m)  \u001b[38;5;66;03m# For an 80/20 split\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(X))\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1288\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1250\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GrITPythonEnv/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GrITPythonEnv/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "relu = ReLULayer()\n",
    "mlp1 = MLP(relu, 2, [50, 30])\n",
    "\n",
    "optimize_MLP_fancypants(mlp1, X_train, y_train, 0.01, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = mlp1.predict(X_test)\n",
    "print(f\"Test Accuracy: {evaluate_accuracy(y_test, predictions_test)*100}%\")\n",
    "\n",
    "predictions_test = mlp1_fitted.predict(X_test)\n",
    "print(f\"Test Accuracy: {evaluate_accuracy(y_test, predictions_test)*100}%\")\n",
    "\n",
    "relu = ReLULayer()\n",
    "mlp2 = MLP(relu, 2, [50,30])\n",
    "predictions_test = mlp2.predict(X_test)\n",
    "print(f\"Test Accuracy: {evaluate_accuracy(y_test, predictions_test)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GrITPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
